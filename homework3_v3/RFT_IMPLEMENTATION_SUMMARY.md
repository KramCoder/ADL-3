# RFT Implementation Summary

## Changes Made to Implement RFT Data Generation and Training

### Overview
Modified the SFT training pipeline to use RFT (Rejection Sampling Fine-Tuning) data generation with the HuggingFaceTB/SmolLM2-1.7B-Instruct model, as specified in the homework requirements.

## Changes Made

### 1. Modified `homework/cot.py`
**Purpose**: Use the 1.7B model for better RFT data generation

**Changes**:
- Updated `CoTModel.__init__()` to default to `HuggingFaceTB/SmolLM2-1.7B-Instruct` instead of the 360M model
- This ensures that when `datagen.py` uses `CoTModel`, it uses the larger, more capable model
- The 1.7B model provides better reasoning capabilities for generating diverse, correct completions

```python
class CoTModel(BaseLLM):
    def __init__(self, *args, **kwargs):
        # Use the 1.7B model for better RFT data generation
        if 'checkpoint' not in kwargs:
            kwargs['checkpoint'] = "HuggingFaceTB/SmolLM2-1.7B-Instruct"
        super().__init__(*args, **kwargs)
```

### 2. Modified `homework/sft.py`
**Purpose**: Train on RFT data (question + reasoning pairs) instead of simple answer format

**Changes**:

#### a) Updated `format_example()` function
- Added optional `reasoning` parameter to handle both RFT and simple data formats
- When `reasoning` is provided (RFT data):
  - Uses the full reasoning text which includes the step-by-step explanation and answer tags
  - Validates that answer tags are present
- When `reasoning` is not provided:
  - Falls back to simple answer format: `<answer>{value}</answer>`

```python
def format_example(prompt: str, answer: float, reasoning: str = None) -> dict[str, str]:
    """
    Construct a question / answer pair for RFT training.
    If reasoning is provided (RFT data), use it. Otherwise, use simple answer format.
    
    RFT Format: question + reasoning (which includes answer)
    Simple Format: question + <answer>{answer}</answer>
    """
    if reasoning is not None:
        # RFT format: reasoning already contains the answer tags
        # ...
        return {
            "question": prompt.strip(),
            "answer": reasoning,  # Full reasoning text with answer tags
        }
    else:
        # Simple answer format (original SFT)
        # ...
        return {
            "question": prompt.strip(),
            "answer": f"<answer>{formatted_answer}</answer>",
        }
```

#### b) Updated `train_model()` function
- Modified dataset loading to prioritize RFT data if available
- Checks for `data/rft.json` file:
  - If found: Loads RFT data with format `[question, answer, reasoning]`
  - If not found: Falls back to regular train dataset with simple answer format
- Added validation checks for RFT data quality:
  - Verifies the expected format (3 elements per entry)
  - Checks for presence of answer tags in reasoning
  - Warns if dataset size is below target (850-900+ examples)
- Created `RFTDataset` wrapper class to make RFT data compatible with `TokenizedDataset`

## RFT Data Format

The RFT data generated by `datagen.py` has the following format:

```json
[
  [
    "How many gram are there per 6 kg?",
    6000.0,
    "1 kg = 1000 grams. 6 * 1000 = <answer>6000</answer>"
  ],
  ...
]
```

Where each entry is:
1. **Question** (string): The unit conversion question
2. **Answer** (float): The correct numerical answer
3. **Reasoning** (string): Chain-of-thought reasoning including the answer in tags

## Workflow

### Step 1: Generate RFT Data
```bash
python -m homework.datagen data/rft.json
```

This will:
1. Use `CoTModel` with the 1.7B model
2. Generate 10-20 completions per training question with temperature > 0
3. Select completions with correct answers
4. Save to `data/rft.json`
5. Target: 850-900+ question/reasoning pairs with 90%+ success rate

### Step 2: Train SFT Model on RFT Data
```bash
python -m homework.sft train
```

This will:
1. Load RFT data from `data/rft.json` (if available)
2. Train the model on question + reasoning pairs
3. The model learns both the reasoning process AND the correct answer format
4. Save the trained LoRA adapter to `homework/sft_model/`

### Step 3: Test the Model
```bash
python -m homework.sft test
```

## Benefits of RFT Approach

1. **Better Reasoning**: The model learns to show its work before providing answers
2. **Higher Accuracy**: Training on correct reasoning chains improves generalization
3. **Combines Strengths**: Merges Chain-of-Thought reasoning with Supervised Fine-Tuning
4. **Offline RL**: Uses rejection sampling to filter for correct answers without online training

## Backward Compatibility

The implementation maintains backward compatibility:
- If `data/rft.json` doesn't exist, training falls back to regular train data
- The `format_example()` function handles both formats automatically
- Existing tests and evaluation code continue to work unchanged

## Model Requirements

- **Data Generation**: Uses HuggingFaceTB/SmolLM2-1.7B-Instruct for better completions
- **Training**: Uses the base HuggingFaceTB/SmolLM2-360M-Instruct with LoRA adapters
- **File Size**: LoRA adapter stays within 20MB limit for SFT (can go up to 50MB for RFT)
