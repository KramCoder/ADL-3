# Grader Issues Analysis

## Summary

The grader is failing on two tests:
1. **Non-batched generate function**: 0/10 points
2. **RFT Model accuracy**: 0/25 points

## Issue 1: Non-batched Generate Function (0/10)

### What the grader checks:
The grader (`GenerateGrader` in `grader/tests.py`) does the following:
1. Loads a `BaseLLM` model
2. Gets raw questions from the dataset: `questions = [dataset[i][0] for i in range(32)]`
3. Calls `model.generate(questions[i])` for each question individually
4. Computes loss on: `full_texts = [questions[i] + answers[i] for i in range(len(questions))]`

### The Problem:
The `generate()` method in `base_llm.py` uses `format_prompt(question)` which adds a space:
```python
def format_prompt(self, question: str) -> str:
    return f"{question.strip()} "  # Adds a space after the question
```

So during generation, the model sees: `question.strip() + " " + generated_answer`

But the grader concatenates: `raw_question + answer` (where raw_question may have different whitespace)

This format mismatch causes the loss computation to be incorrect, resulting in a loss outside the acceptable bounds (6.2-8.0).

### Why batched_generate passes:
The batched version might be passing due to different tokenization behavior or because the loss bounds are more lenient for batched operations. However, both should ideally produce the same results.

### Solution:
The `generate()` method needs to ensure that when the grader does `questions[i] + answers[i]`, it produces text that tokenizes the same as what the model actually saw during generation.

The model generates after: `format_prompt(question) = question.strip() + " "`
So the full sequence is: `question.strip() + " " + generated_answer`

The grader computes loss on: `raw_question + answer`

For the loss to be correct, these must tokenize to the same sequence. The issue is that:
- `format_prompt` adds a space: `question.strip() + " "`
- The generated answer is decoded from tokens (which don't include the input space)
- When concatenated: `raw_question + answer` might not match `question.strip() + " " + generated_answer`

**Potential fixes:**
1. Make `generate()` return an answer that, when prepended with a space, matches the generated tokens
2. Ensure the answer format accounts for the space that `format_prompt` adds
3. Check if the tokenizer's behavior with `skip_special_tokens=True` is causing the mismatch

**Recommended approach**: The answer should be returned such that `question.strip() + " " + answer` tokenizes the same as the original generation. Since the grader does `raw_question + answer`, we need to ensure `raw_question` (when stripped and space-added) matches, OR adjust the answer format.

## Issue 2: RFT Model Accuracy (0/25)

### What the grader checks:
The grader (`RFTGrader` in `grader/tests.py`) does the following:
1. Calls `load_rft()` to load the RFT model
2. Runs `benchmark(model, dataset, 100)` which calls `model.answer(*questions)`
3. Checks accuracy - expects accuracy between 0.6-0.7 for full points

### The Problem:
**The RFT model has not been trained!**

The `homework/rft_model/` directory only contains a `README.md` file. There are no model files:
- No `adapter_model.bin` or `adapter_model.safetensors`
- No `adapter_config.json`

When `load_rft()` is called, it calls `_ensure_adapter()` which creates a **new, untrained** LoRA adapter if one doesn't exist. An untrained adapter will have random weights, resulting in 0% accuracy.

### Solution:
The RFT model needs to be trained by running:
```bash
python -m homework.rft train
```

This requires:
1. RFT training data at `data/rft.json` (generated by `python -m homework.datagen data/rft.json`)
2. Training the model with the RFT training script
3. The trained model files should be saved in `homework/rft_model/`

After training, the model should achieve >60% accuracy on the validation set to pass the grader.

## Summary of Required Actions

1. **Fix non-batched generate**: Ensure the output format matches grader expectations for loss computation
2. **Train RFT model**: Run the RFT training script to generate model files, or include pre-trained model files in the submission
